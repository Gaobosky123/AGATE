{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from utils.load_data import load_dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # 进度条显示\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "# load pre-trainede CLIP model\n",
    "victim ='ViT-B-16-quickgelu'\n",
    "pretrained = \"openai\"\n",
    "model, _, transform = open_clip.create_model_and_transforms(victim, pretrained=pretrained)\n",
    "model = model.to(device)\n",
    "tokenizer = open_clip.get_tokenizer(victim)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# from torchvision import models\n",
    "# class StudentImageEncoder(nn.Module):\n",
    "#     def __init__(self, backbone='mobilenet_v3_small', embed_dim=512):\n",
    "#         super().__init__()\n",
    "#         if backbone == 'mobilenet_v3_small':\n",
    "#             self.backbone = models.mobilenet_v3_small(pretrained=True)\n",
    "#             in_features = self.backbone.classifier[-1].in_features\n",
    "#             self.backbone.classifier[-1] = nn.Linear(in_features, embed_dim)\n",
    "#         elif backbone == 'efficientnet_b0':\n",
    "#             self.backbone = models.efficientnet_b0(pretrained=True)\n",
    "#             in_features = self.backbone.classifier[-1].in_features\n",
    "#             self.backbone.classifier[-1] = nn.Linear(in_features, embed_dim)\n",
    "#         elif backbone == 'resnet18':\n",
    "#             self.backbone = models.resnet18(pretrained=True)\n",
    "#             self.backbone.fc = nn.Linear(512, embed_dim)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported backbone\")\n",
    "#         self.embed_dim = embed_dim\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.backbone(x)\n",
    "    \n",
    "# student = StudentImageEncoder(backbone='mobilenet_v3_small').train().to(device)\n",
    "# path_st = \"student_flickr30k_clip.pth\"\n",
    "# student.load_state_dict(torch.load(path_st, map_location=device))\n",
    "# student.eval()\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "model_KD = CLIPModel.from_pretrained('/root/autodl-tmp/AdvCLIP/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M')\n",
    "processor_KD = CLIPProcessor.from_pretrained('/root/autodl-tmp/AdvCLIP/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M')\n",
    "\n",
    "model_KD = model_KD.to(device)\n",
    "\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=512):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def forward(self, image_embeddings, text_embeddings):\n",
    "        logits_per_image = self.cosine_similarity(image_embeddings.unsqueeze(1), text_embeddings.unsqueeze(0)) / self.temperature\n",
    "        labels = torch.arange(len(image_embeddings), device=image_embeddings.device)\n",
    "        loss_i = nn.CrossEntropyLoss()(logits_per_image, labels)\n",
    "        loss_t = nn.CrossEntropyLoss()(logits_per_image.T, labels)\n",
    "        return (loss_i + loss_t) / 2\n",
    "    \n",
    "# load cross-modal dataset\n",
    "dataset ='pascal'\n",
    "batch_size = 16\n",
    "dataloaders = load_dataset(dataset, batch_size)\n",
    "#train_loader = dataloaders['train']\n",
    "test_loader = dataloaders['test']\n",
    "\n",
    "mlp_model = SimpleMLP().train().to(device)\n",
    "\n",
    "path = \"output/Module/mlp_pascal_ViT-B-16-quickgelu_200_100.pth\"\n",
    "mlp_model.load_state_dict(torch.load(path, map_location=device))\n",
    "mlp_model.eval()\n",
    "\n",
    "from pathlib import Path\n",
    "uap_root = os.path.join('output', 'uap', 'gan_patch', \"ViT-B16\", str(dataset),str(0.03))\n",
    "uap_path = [Path(uap_root) / ckpt for ckpt in os.listdir(Path(uap_root)) if ckpt.endswith(\"20.pt\")][0]\n",
    "uap = torch.load(uap_path)\n",
    "\n",
    "\n",
    "def patch_initialization(patch_type='rectangle'):\n",
    "    noise_percentage = 0.03\n",
    "    image_size = (3, 224, 224)\n",
    "    if patch_type == 'rectangle':\n",
    "        mask_length = int((noise_percentage * image_size[1] * image_size[2])**0.5)\n",
    "        patch = np.random.rand(image_size[0], mask_length, mask_length)\n",
    "    return patch\n",
    "\n",
    "def mask_generation(patch):\n",
    "    image_size = (3, 224, 224)\n",
    "    applied_patch = np.zeros(image_size)\n",
    "    x_location = image_size[1] - 14 - patch.shape[1]\n",
    "    y_location = image_size[1] - 14 - patch.shape[2]\n",
    "    applied_patch[:, x_location: x_location + patch.shape[1], y_location: y_location + patch.shape[2]] = patch\n",
    "    mask = applied_patch.copy()\n",
    "    mask[mask != 0] = 1.0\n",
    "    return mask, applied_patch ,x_location, y_location\n",
    "\n",
    "patch = patch_initialization()\n",
    "#mask, applied_patch, x, y = mask_generation(patch)\n",
    "mask, applied_patch, x, y = mask_generation(patch)\n",
    "applied_patch = torch.from_numpy(applied_patch)\n",
    "mask = torch.from_numpy(mask)\n",
    "\n",
    "def cal_sim(vector_0, vector_1):\n",
    "    '''\n",
    "    Calculate the cos sim and pairwise distance\n",
    "    :param vector_0:\n",
    "    :param vector_1:\n",
    "    :return: cos_sim, pair_dis\n",
    "    '''\n",
    "    cos_sim_f = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    pair_dis_f = torch.nn.PairwiseDistance(p=2)\n",
    "    cos_sim = cos_sim_f(vector_0, vector_1)\n",
    "    pair_dis = pair_dis_f(vector_0, vector_1)\n",
    "    return cos_sim, pair_dis\n",
    "\n",
    "start_time_2 = time.time()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "Size_Trigger = 128\n",
    "# batch_size = 1\n",
    "def evaluate_Verify(model, clip, Trigger_mat, test_loader, device):\n",
    "    \n",
    "    list_origin_cos_sim = []\n",
    "    list_origin_pair_dis = []\n",
    "    list_Trigger_cos_sim = []\n",
    "    list_Trigger_pair_dis = []\n",
    "    list_cos_sim_Trigger= []\n",
    "    list_pair_dis_Trigger = []\n",
    "\n",
    "    round = Size_Trigger/batch_size\n",
    "    for i, (batch_images, batch_texts, inds, IDs) in enumerate(test_loader):\n",
    "        if i > (round-1):\n",
    "            break\n",
    "        batch_images = batch_images.squeeze().to(device)\n",
    "        # tokenize all texts in the batch\n",
    "        #batch_texts_tok = tokenizer([text for i, texts in enumerate(batch_texts) for text in texts]).to(device)\n",
    "        batch_texts_tok = batch_texts.squeeze().to(device)\n",
    "      \n",
    "        # store the index of image for each text\n",
    "        target = inds.to(device)\n",
    "\n",
    "        image_adv = torch.mul(mask.type(torch.FloatTensor), uap.type(torch.FloatTensor)) + \\\n",
    "            torch.mul(1 - mask.expand(batch_images.shape).type(torch.FloatTensor), batch_images.type(torch.FloatTensor))\n",
    "        p_data = image_adv.clone()\n",
    "        # compute the embedding of images and texts\n",
    "        with torch.no_grad():\n",
    "\n",
    "            image_features = model.get_image_features(batch_images)\n",
    "            image_features_T = model.get_image_features(p_data.to(device))\n",
    "            text_features = model.get_text_features(batch_texts_tok)\n",
    "\n",
    "\n",
    "            origin_image_features = image_features\n",
    "            T_image_features = image_features_T\n",
    "            origin_text_features = text_features\n",
    "            origin_image_features /= origin_image_features.norm(dim=-1, keepdim=True)\n",
    "            # Trigger\n",
    "            T_image_features /= T_image_features.norm(dim=-1, keepdim=True)\n",
    "            origin_text_features /= origin_text_features.norm(dim=-1, keepdim=True)\n",
    "            origin_cos_sim, origin_pair_dis = cal_sim(origin_image_features, origin_text_features)\n",
    "            Trigger_cos_sim, Trigger_pair_dis = cal_sim(T_image_features, origin_text_features)\n",
    "\n",
    "            list_origin_cos_sim.append(origin_cos_sim.cpu().tolist())\n",
    "            list_origin_pair_dis.append(origin_pair_dis.cpu().tolist())\n",
    "            list_Trigger_cos_sim.append(Trigger_cos_sim.cpu().tolist())\n",
    "            list_Trigger_pair_dis.append(Trigger_pair_dis.cpu().tolist())\n",
    "\n",
    "            similarity_1 = 100. * (origin_image_features @ origin_text_features.T)\n",
    "            p_similarity_1 = 100. * (T_image_features @ origin_text_features.T)\n",
    "\n",
    "            probs_1 = F.softmax(similarity_1, dim=-1).max(-1)[1]\n",
    "            p_probs_1 = F.softmax(p_similarity_1, dim=-1).max(-1)[1]\n",
    "\n",
    "            print(f'CLIP Model = {victim}')\n",
    "            print(f'Trigger = {uap_path}')\n",
    "            print(f'Module = {path}')\n",
    "            print(\"after CLIP\")\n",
    "            print(\"Origin: cos similarity: %lf, pair distance: %lf\" % (float(origin_cos_sim.mean()), float(origin_pair_dis.mean())))\n",
    "            print(\"Trigger_mat: cos similarity: %lf, pair distance: %lf\" % (float(Trigger_cos_sim.mean()), float(Trigger_pair_dis.mean())))\n",
    "\n",
    "  \n",
    "            image_features = Trigger_mat(image_features)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            # Trigger\n",
    "            image_features_T = Trigger_mat(image_features_T)\n",
    "            image_features_T /= image_features_T.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            text_features = Trigger_mat(text_features)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "            similarity = 100. * (image_features @ text_features.T)\n",
    "            p_similarity = 100. * (image_features_T @ text_features.T)\n",
    "\n",
    "\n",
    "            probs = F.softmax(similarity, dim=-1).max(-1)[1]\n",
    "            p_probs = F.softmax(p_similarity, dim=-1).max(-1)[1]\n",
    "\n",
    "\n",
    "            cos_sim_origin, pair_dis_origin = cal_sim(image_features, text_features)\n",
    "            cos_sim_Trigger, pair_dis_Trigger = cal_sim(image_features_T, text_features)\n",
    "\n",
    "            list_cos_sim_Trigger.append(cos_sim_Trigger.cpu().tolist())\n",
    "            list_pair_dis_Trigger.append( pair_dis_Trigger.cpu().tolist())\n",
    "\n",
    "            print(\"after Module\")\n",
    "            print(\"Origin: cos similarity: %lf, pair distance: %lf\" % (float(cos_sim_origin.mean()), float(pair_dis_origin.mean())))\n",
    "            print(\"Trigger_mat: cos similarity: %lf, pair distance: %lf\" % (float(cos_sim_Trigger.mean()), float(pair_dis_Trigger.mean())))\n",
    "            # print(\"delta_module: cos similarity: %lf, pair distance: %lf\" % (delta_cos_module, delta_euc_module))\n",
    "\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "    return list_origin_cos_sim, list_origin_pair_dis,     list_Trigger_cos_sim,     list_Trigger_pair_dis,     list_cos_sim_Trigger,     list_pair_dis_Trigger\n",
    "\n",
    "list_A0, list_B0, list_A1, list_B1, list_A2, list_B2= evaluate_Verify(model_KD, model, mlp_model,test_loader,device)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time_2\n",
    "\n",
    "print(\"total_time = \", total_time)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
